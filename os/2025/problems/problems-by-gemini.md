* **Shell Programming:** grep, sed, awk, find, sort, uniq, cut, cat, etc.  
* **UNIX Processes:** fork, wait, exit, exec, signal, kill  
* **UNIX Inter-Process Communication (IPC):** pipe, FIFO  
* **POSIX Threads:** pthread\_create, pthread\_join, etc.  
* **Thread Synchronization:** mutexes, conditional variables, RW locks, barriers

### **Only Shell Programming**

Problems solvable using standard UNIX shell commands and utilities.

**Easy:**

1. **Count Total Log Entries:**  
   * *Problem:* Determine the total number of lines in the log file.  
   * *Hints:* Use cat and wc.  
2. **Filter by Log Level:**  
   * *Problem:* Extract all log entries where the level is "ERROR".  
   * *Hints:* Use grep to match the specific string " ERROR ".  
3. **Find Entries for a Specific User:**  
   * *Problem:* Find all log entries generated by a user named "\[user:user1\]".  
   * *Hints:* Use grep to match the specific user string.  
4. **Extract Timestamps:**  
   * *Problem:* Extract only the datetime\_seconds (the first field) from each log entry.  
   * *Hints:* Use cut with the appropriate delimiter and field number.  
5. **Count Entries per Log Level:**  
   * *Problem:* Count how many entries exist for each unique log level (INFO, ERROR, WARNING, PERF).  
   * *Hints:* Use cut to extract the level field, then sort and uniq \-c.

**Medium:**

1. **Find Error Messages Containing Keyword:**  
   * *Problem:* Find all "ERROR" level log entries where the \[err-msg:...\] part contains the keyword "denied".  
   * *Hints:* Combine grep for the level and another grep (or awk) to search within the message part.  
2. **Count Requests per Application:**  
   * *Problem:* Count how many log entries have level "INFO" and method "process\_request" for each distinct application.  
   * *Hints:* Use grep to filter for level and method, then awk to extract the application field, followed by sort and uniq \-c.  
3. **Identify Users with Failed Logins:**  
   * *Problem:* List all unique users who have "ERROR" level entries with the method "authenticate\_user".  
   * *Hints:* Use grep to filter for level and method, then awk to extract the user field, followed by sort \-u.  
4. **Extract Performance Metrics:**  
   * *Problem:* For all "PERF" level entries, extract the method, system, and the numeric duration from the message (e.g., "Request processed in 123ms").  
   * *Hints:* Use grep for "PERF", then awk to extract the required fields and parse the message string using regular expressions or field splitting.  
5. **Filter Entries within a Time Range:**  
   * *Problem:* Extract all log entries that occurred between two specific datetime\_seconds values (e.g., entries with timestamp \>= 1678886400 and \<= 1678887000).  
   * *Hints:* Use awk to compare the first field (timestamp) with the given range.

**Hard:**

1. **Analyze Error Stack Traces:**  
   * *Problem:* For "ERROR" entries containing a simulated stack trace in the \[err-msg:...\], extract the top-most method and line number from the stack trace lines (lines starting with \\tat or similar).  
   * *Hints:* Use grep to find error entries with stack traces. Use sed or awk with multi-line pattern matching or careful line processing to isolate the stack trace part and extract the first frame's details.  
2. **Calculate Average Request Duration per Application:**  
   * *Problem:* For "PERF" level entries with method "process\_request", calculate the average duration (the numeric value in the message like "123ms") for each unique application.  
   * *Hints:* Use grep to filter relevant lines. Use awk to extract the application and the numeric duration, then use awk's associative arrays and arithmetic capabilities to sum durations and count entries per application, finally calculating the average.  
3. **Identify Applications with High Error Rates:**  
   * *Problem:* For each application, calculate the total number of log entries and the number of "ERROR" entries. Report applications where the percentage of "ERROR" entries exceeds a threshold (e.g., 5%).  
   * *Hints:* Use awk to count total entries and error entries per application using associative arrays. After processing the file, iterate through the arrays to calculate percentages and print results based on the threshold.  
4. **Track User Activity Sequence:**  
   * *Problem:* For a specific user (e.g., "\[user:user456\]"), list their log entries in chronological order (based on datetime\_seconds), showing the timestamp, level, method, and a truncated message.  
   * *Hints:* Use grep to filter for the user. Use sort \-n on the first field (timestamp). Use awk or cut to format the output fields.  
5. **Find Correlated Events:**  
   * *Problem:* Identify pairs of "INFO" (message containing "Request received from") and "PERF" (message containing "Request processed in") entries for the *same user* and *same application* where the PERF entry's timestamp is within a short time window (e.g., 10 seconds) after the INFO entry's timestamp. Report the user, application, and the calculated duration (PERF timestamp \- INFO timestamp).  
   * *Hints:* This is challenging with pure shell. awk is the most suitable tool. You'll need awk to store relevant INFO entries (timestamp, user, app) in an array or hash map keyed by a combination of user and app. When a PERF entry is encountered, look up the corresponding INFO entry, check the timestamp difference, and calculate/report if the criteria are met. Requires careful handling of state in awk.

### **Only C Programming with Processes**

Problems solvable using C language with UNIX process management system calls.

**Easy:**

1. **Read and Print Log File:**  
   * *Problem:* Write a C program that opens the log file, reads it line by line, and prints each line to standard output.  
   * *Hints:* Use fopen, getline (or fgets), printf, fclose.  
2. **Filter by Level (Single Process):**  
   * *Problem:* Write a C program that reads the log file and prints only lines where the level field is "ERROR".  
   * *Hints:* Read lines, use string searching functions (strstr or sscanf with format string) to check for " ERROR ", then print if it matches.  
3. **Count Total Lines (Single Process):**  
   * *Problem:* Write a C program that reads the log file and counts the total number of lines.  
   * *Hints:* Read lines in a loop and increment a counter.  
4. **Basic Fork and Exec:**  
   * *Problem:* Write a C program that fork()s a child process. The child process should then use execvp() or execlp() to run a simple shell command like cat logfile.txt or grep ERROR logfile.txt. The parent should wait for the child to finish.  
   * *Hints:* Use fork(), check the return value (0 for child, child PID for parent, \-1 for error). Use execvp() or execlp() in the child. Use wait(NULL) in the parent.  
5. **Simple Parent-Child Communication (Exit Status):**  
   * *Problem:* Write a C program where the parent fork()s a child. The child counts the number of "WARNING" lines in the log file and exits with that count as its exit status. The parent uses wait() to get the child's exit status and prints the count.  
   * *Hints:* Use fork(). In the child, count lines and use exit(count). In the parent, use wait(\&status), then use WEXITSTATUS(status) to get the exit value.

**Medium:**

1. **Distribute Line Counting:**  
   * *Problem:* Create a parent process that fork()s N child processes. Each child process is responsible for counting lines in a *specific, non-overlapping section* of the log file. The parent waits for all children and sums their counts.  
   * *Hints:* The parent needs to determine the file size and divide it into N sections. Pass the file path and the start/end byte offsets for each section as command-line arguments to the child processes via exec. Children use fseek or lseek to go to their section and read/count lines within that range. Parent uses wait for each child.  
2. **Parallel Filtering:**  
   * *Problem:* Create a parent process that fork()s a child process for each log level (INFO, ERROR, WARNING, PERF). Each child process reads the entire log file and writes only the lines matching its assigned level to a separate temporary file. The parent waits for all children.  
   * *Hints:* Parent fork()s multiple times. Each child receives its level as an argument via exec. Children open the log file for reading and a unique temporary file for writing. They filter and write lines. Parent uses wait for all children.  
3. **Inter-Process Communication (Pipe) for Counts:**  
   * *Problem:* Write a C program where the parent fork()s a child. The parent creates a pipe(). The child counts "ERROR" lines and writes the count (as a string or integer) to the write end of the pipe. The parent reads from the read end of the pipe and prints the count.  
   * *Hints:* Use pipe() before fork(). In the parent, close the write end and read from the read end. In the child, close the read end and write to the write end. Use read() and write(). Remember to close unused pipe ends in both processes.  
4. **Basic Signal Handling:**  
   * *Problem:* Write a C program (single process) that reads the log file. Implement a signal handler for SIGINT (Ctrl+C). When SIGINT is received, the program should print a message like "Received SIGINT, stopping..." and exit gracefully.  
   * *Hints:* Use signal(SIGINT, handler\_function) or sigaction. The handler function should set a flag (e.g., a global volatile variable). The main loop should check this flag periodically and break if set.  
5. **Process per Application:**  
   * *Problem:* Create a parent process that identifies all unique applications in the log file. For each unique application, it fork()s a child process. Each child process reads the log file and counts entries *only* for its assigned application. The parent waits for all children. (Getting counts back requires IPC \- see next section).  
   * *Hints:* First pass through the file to find unique applications. Then loop, fork()ing a child for each app. Pass the app name to the child via exec. Children open and read the log, filtering by the app name.

**Hard:**

1. **Parallel Log Parsing and Aggregation:**  
   * *Problem:* Divide the log file into sections. fork() multiple child processes, each assigned a section. Each child parses its section, counts entries per log level and per application *within its section*. Children send their partial counts back to the parent via pipes or FIFOs. The parent reads from all children and aggregates the final counts.  
   * *Hints:* Parent calculates sections and creates N pipes (one for each child). Parent fork()s N children, passing section info and pipe write end FD via exec. Children read their section, parse lines (e.g., using sscanf or string manipulation), maintain local counts (e.g., in arrays or structs), and write their local counts to the pipe. Parent reads from all pipe read ends and sums the counts. Requires careful synchronization if using shared memory (more advanced IPC).  
2. **Robust Error Handling with Signals:**  
   * *Problem:* Implement a multi-process log processing system (e.g., parent spawning workers). Implement signal handlers for SIGINT, SIGTERM, and SIGCHLD in the parent. SIGINT/SIGTERM should trigger a graceful shutdown (e.g., sending a custom signal to children or closing pipes to signal completion). SIGCHLD should handle child termination (clean exit or crash), potentially logging the event and deciding whether to restart.  
   * *Hints:* Use sigaction for signal handling. Handlers set flags or use kill() to signal other processes. Parent's SIGCHLD handler needs to call waitpid in a loop to reap all terminated children and check their status (WIFEXITED, WEXITSTATUS, WIFSIGNALED, WTERMSIG).  
3. **Dynamic Process Creation:**  
   * *Problem:* Write a C program that monitors a directory for new log files or changes to an existing log file (e.g., using inotify on Linux, or periodic polling). When new log data is detected, fork() a new child process to process *only* the new data. Manage a pool of active worker processes.  
   * *Hints:* Use inotify\_init, inotify\_add\_watch, read from the inotify FD (Linux). For polling, use stat and check file size/modification time periodically. When new data is found, fork() and pass the file path and the offset of the new data to the child. Parent needs to track active children and potentially limit the number of running processes.  
4. **Distributed Filtering with FIFOs:**  
   * *Problem:* Create a parent process that creates several Named Pipes (FIFOs). fork() multiple child processes, each responsible for filtering a specific log level or application. The parent could potentially read the log and distribute lines to the appropriate child's FIFO (requires careful synchronization or a single reader process). Alternatively, each child opens the *same* log file and its *own* FIFO for writing. They filter and write matching lines to their respective FIFOs. A final parent process reads from all FIFOs concurrently to merge the results.  
   * *Hints:* Use mkfifo() to create FIFOs. Processes open FIFOs using open(). Remember to handle blocking/non-blocking reads/writes. Multiple processes reading the *same* file concurrently requires careful lseek or other synchronization to avoid reading the same data (often simpler to have one reader process distribute).  
5. **Process Pool for Task Distribution:**  
   * *Problem:* Implement a fixed pool of N worker processes. The main process reads log entries and puts them into a shared task queue (e.g., implemented using pipes or a shared memory segment with mutexes/semaphores). Worker processes repeatedly take a task (log line or section) from the queue, process it (e.g., count levels, extract data), and potentially put results into another shared result queue.  
   * *Hints:* Requires robust IPC for the shared queue (pipe for simple queues, shmget/shmat with semaphores or mutexes for shared memory queue). Workers loop, trying to get a task (potentially blocking if the queue is empty). Parent adds tasks and manages the worker pool (e.g., restarting failed workers).

### **Only C Programming with Processes and Communication between Parent and Child**

Problems focused on using C with processes and explicit communication mechanisms like pipes or FIFOs.

**Easy:**

1. **Child Sends Line Count to Parent (Pipe):**  
   * *Problem:* Parent fork()s a child. Parent creates a pipe. Child counts total lines in the log and sends the count through the pipe to the parent. Parent reads and prints the count.  
   * *Hints:* Use pipe(), fork(). Child closes read end, counts, write()s count to write end, closes write end, exit(). Parent closes write end, read()s from read end, closes read end, wait(), printf().  
2. **Parent Sends Filter Criteria to Child (Pipe):**  
   * *Problem:* Parent fork()s a child and creates a pipe. Parent reads a log level (e.g., "WARNING") from user input and sends it through the pipe to the child. The child reads the level from the pipe and then filters the log file, printing only entries matching that level.  
   * *Hints:* Use pipe(), fork(). Parent closes read end, reads user input, write()s criterion to write end, closes write end, wait(). Child closes write end, read()s criterion from read end, closes read end, opens log file, filters based on criterion, prints matching lines, exit().  
3. **Child Reports First Error Line to Parent (Pipe):**  
   * *Problem:* Parent fork()s a child and creates a pipe. Child reads the log file and finds the *first* "ERROR" level line. It sends this entire line through the pipe to the parent. Parent reads and prints the line.  
   * *Hints:* Similar pipe setup. Child reads log, finds first error line, write()s it to the pipe, exit(). Parent reads from the pipe until EOF (child closes write end) and prints the received data.  
4. **Parent Signals Child to Stop (Signal):**  
   * *Problem:* Parent fork()s a child. The child continuously reads and prints the log file in a loop. After a few seconds, the parent sends a SIGTERM signal to the child using kill(). The child should have a signal handler for SIGTERM that causes it to exit gracefully.  
   * *Hints:* Parent fork(), get child PID. Parent sleep(), then kill(child\_pid, SIGTERM). Child uses signal(SIGTERM, handler) or sigaction. Handler sets a flag. Child's read loop checks the flag.  
5. **Child Sends Progress Updates to Parent (Pipe):**  
   * *Problem:* Parent fork()s a child and creates a pipe. Child reads the log file and counts lines. Every 100 lines processed, the child writes its current line count to the pipe. Parent reads these counts from the pipe and prints progress messages.  
   * *Hints:* Pipe setup. Child counts lines in a loop. Inside the loop, if count is a multiple of 100, write() the count to the pipe. Parent reads from the pipe in a loop (potentially non-blocking read or in a separate thread/signal handler if more complex) and prints.

**Medium:**

1. **Multiple Children Send Counts to Parent (Multiple Pipes):**  
   * *Problem:* Parent fork()s N children. Each child is assigned a different log level to count. Each child has its *own* pipe back to the parent. Children count their assigned level and send the total count through their pipe. Parent reads from all N pipes and prints the final counts per level.  
   * *Hints:* Parent creates N pairs of pipes (2\*N file descriptors). When fork()ing each child, it passes the correct pipe FDs. Each child closes all unused pipe ends and uses its designated pipe to write its count. Parent closes all child write ends and reads from all child read ends.  
2. **Parent Distributes Work (File Sections) via Pipes:**  
   * *Problem:* Parent reads the log file and divides it into N sections (e.g., byte offsets). Parent fork()s N children. Parent sends the start and end offsets for each child's section through a pipe (or separate pipes). Children read their section info from the pipe, process their section of the file, and exit.  
   * *Hints:* Parent calculates offsets. Parent creates pipe. Parent fork()s N children. Parent writes offset pairs to the pipe. Children read from the pipe, use lseek or fseek to navigate the log file, process their section, and exit(). Parent waits.  
3. **Child Reports Errors with Context (Pipe):**  
   * *Problem:* Parent fork()s a child and creates a pipe. Child reads the log file. When it finds an "ERROR" entry, it sends the entire error line *and* the preceding INFO line (if available) through the pipe to the parent. Parent reads and prints the error and its context.  
   * *Hints:* Child needs to buffer the last seen INFO line. When an ERROR is found, write the buffered INFO line (if any) and the ERROR line to the pipe. Parent reads line by line from the pipe.  
4. **Parent Manages Worker Pool (Signals \+ Wait):**  
   * *Problem:* Parent maintains a pool of M worker processes. Parent reads log lines. When a line needs processing (e.g., an "ERROR" line), the parent finds an idle worker process and signals it (e.g., with a custom signal like SIGUSR1) to process the line. The worker process, upon receiving the signal, reads the line (passed via another mechanism, perhaps a shared buffer or another pipe), processes it, and signals the parent when done. Parent uses waitpid with WNOHANG to check worker status without blocking.  
   * *Hints:* More complex. Parent needs to track worker PIDs and their status (idle/busy). Requires a mechanism to pass the actual log line data to the signaled worker (pipes or shared memory). Workers need signal handlers. Parent's main loop reads log and manages workers.  
5. **Distributed Counting with FIFOs:**  
   * *Problem:* Parent creates a FIFO. Parent fork()s multiple children. Each child counts a specific type of entry (e.g., count INFO, count ERROR, count WARNING). Each child writes its count and type (e.g., "INFO 123") to the *same* FIFO. Parent reads from the FIFO, aggregates the counts by type, and prints the final totals.  
   * *Hints:* Use mkfifo() to create the FIFO. All children open the FIFO's write end. Parent opens the FIFO's read end. Children write formatted strings. Parent reads strings, parses them, and updates counts. Requires handling potential interleaving of writes from multiple children (though write to a pipe/FIFO is often atomic for small amounts of data).

**Hard:**

1. **Parallel Parsing and Aggregation with Structured IPC:**  
   * *Problem:* Divide the log file. fork() N children. Each child parses its section, extracts key fields (timestamp, level, app, user, method, duration for PERF, error message for ERROR), and performs local aggregation (e.g., counts per app/level, sum of durations per app). Children send structured data (e.g., a custom struct or formatted string containing aggregated results) through pipes or FIFOs back to the parent. Parent reads the structured data from all children and performs final aggregation.  
   * *Hints:* Children define a struct or consistent string format for their partial results. Use write() to send this data. Parent uses read() to receive and parse the structured data. Requires careful handling of partial reads and potentially using select or poll if reading from multiple pipes/FIFOs concurrently.  
2. **Fault-Tolerant Worker Pool (Signals \+ IPC):**  
   * *Problem:* Implement a parent process managing a pool of worker children. Parent reads log lines and sends them to workers via pipes. If a worker process crashes unexpectedly (detected by SIGCHLD handler), the parent should restart a new worker and potentially re-assign the task that the crashed worker was handling.  
   * *Hints:* Parent needs a SIGCHLD handler that calls waitpid with WNOHANG in a loop. Track worker PIDs. Maintain a queue of tasks. When a worker is spawned, assign it a task. If a worker exits abnormally, add its task back to the queue and spawn a new worker. Parent sends tasks via pipes.  
3. **Dynamic Workload Distribution (Pipes \+ Signals):**  
   * *Problem:* Parent process reads the log file continuously (e.g., using tail \-f piped into the parent's stdin). Parent fork()s a dynamic number of worker children based on the incoming log rate. Parent distributes new log lines to available workers via pipes. Workers process lines and send results back via another set of pipes. Parent manages the worker pool size dynamically.  
   * *Hints:* Parent reads from stdin (the piped tail). Use select or poll to handle input from stdin and from worker result pipes. Maintain a list of worker PIDs and their pipe FDs. If input rate is high and workers are busy, fork() new workers (up to a limit). If input rate is low and workers are idle, kill() some workers. Requires careful pipe management and synchronization.  
4. **Concurrent Analysis with Shared Memory (Advanced IPC):**  
   * *Problem:* (Requires shared memory concepts, often taught after basic IPC) Use shmget and shmat to create a shared memory segment accessible by multiple processes. Implement shared data structures (e.g., counters, lists of error info) within shared memory, protected by process-shared mutexes or semaphores (pthread\_mutexattr\_setpshared, sem\_open). Parent and child processes access and update these shared structures concurrently.  
   * *Hints:* Use shared memory functions. Initialize mutexes/semaphores in shared memory. Processes acquire locks before accessing shared data and release them afterward. Requires careful error handling for shared memory and synchronization primitives.  
5. **Distributed Task Queue (FIFOs \+ Synchronization Files):**  
   * *Problem:* Implement a task queue using a FIFO. Parent writes tasks (e.g., log line numbers or offsets) to the FIFO. Multiple worker processes read tasks from the *same* FIFO. To coordinate reading from the FIFO and potentially writing results, use separate FIFOs or synchronization files (e.g., lock files created with flock).  
   * *Hints:* mkfifo for the task queue FIFO. Multiple processes opening the read end of a FIFO will each get a unique piece of data. Need a way for workers to signal completion or report results (separate FIFOs or a shared FIFO with message types). Synchronization files (flock) can be used to ensure only one process accesses a shared resource (like updating a result file) at a time.

### **Only C Programming with Threads**

Problems solvable using C language with POSIX threads and synchronization primitives.

**Easy:**

1. **Read and Print Log File (Single Thread):**  
   * *Problem:* Write a C program that creates a single POSIX thread. This thread should open the log file, read it line by line, and print each line to standard output. The main thread should wait for the worker thread to complete.  
   * *Hints:* Use pthread\_create to create the thread. The thread function takes a void\* argument and returns void\*. Use pthread\_join in the main thread. File I/O functions (fopen, getline, printf, fclose) are generally thread-safe for independent file pointers.  
2. **Basic Thread Creation:**  
   * *Problem:* Write a C program that creates 5 threads. Each thread should simply print a message like "Hello from thread \[thread ID\]" and exit. The main thread should wait for all 5 threads to complete.  
   * *Hints:* Use pthread\_create in a loop. Pass a thread ID or index as an argument to the thread function (requires casting int to void\* and back). Use pthread\_join in a loop.  
3. **Simple Shared Counter (Mutex):**  
   * *Problem:* Create multiple threads that concurrently increment a single shared integer counter. Protect access to the counter using a mutex to prevent race conditions. The main thread should print the final value of the counter after all threads finish.  
   * *Hints:* Define a global integer counter and a global pthread\_mutex\_t. Initialize the mutex with pthread\_mutex\_init. In the thread function, use pthread\_mutex\_lock before incrementing the counter and pthread\_mutex\_unlock afterward. Destroy the mutex with pthread\_mutex\_destroy in the main thread.  
4. **Read and Store in Memory (Single Thread):**  
   * *Problem:* Create a single thread that reads the entire log file into a dynamically allocated array of strings or a linked list in memory. The main thread waits for the worker thread. (Accessing this data from other threads would require synchronization \- see Medium/Hard).  
   * *Hints:* Thread function allocates memory (malloc, strdup for lines), reads the file, and populates the data structure. The main thread simply waits.  
5. **Thread per Line (Conceptual/Simple):**  
   * *Problem:* (Illustrative, not efficient for large files) Create a new thread for *each* line of the log file. Each thread processes its single line (e.g., prints it or checks its level). The main thread waits for all lines to be processed.  
   * *Hints:* Read lines in the main thread loop. For each line, allocate memory for the line, create a thread, and pass the line data to the thread function. The thread function processes the line and exits. Main thread must pthread\_join each thread (or detach, but joining is simpler for waiting). Resource management (too many threads) is a key challenge here.

**Medium:**

1. **Parallel Filtering (Multiple Threads):**  
   * *Problem:* Create multiple threads. Each thread reads the *entire* log file independently and writes only the lines matching a specific log level to a separate output file (one output file per level/thread). No shared write resource needs mutex if writing to distinct files.  
   * *Hints:* Pass the log file path and the level to filter for as arguments to each thread. Each thread opens the log file for reading and its unique output file for writing.  
2. **Thread Pool for Line Processing:**  
   * *Problem:* Create a fixed pool of N worker threads. A main thread reads log lines and puts them into a thread-safe queue. Worker threads take lines from the queue, parse them (e.g., extract level, app, user), and print summary information. Use mutexes and conditional variables for the thread-safe queue.  
   * *Hints:* Implement a linked list or array for the queue. Use a mutex to protect queue access (enqueue/dequeue). Use a conditional variable to signal workers when a new item is added (producer signals consumer) and potentially to signal the producer if the queue is full. Workers loop, locking the mutex, waiting on the condition variable if the queue is empty, dequeueing, unlocking, and processing.  
3. **Thread-Safe Counter per Level:**  
   * *Problem:* Create multiple threads that read the log file concurrently. Use a shared data structure (e.g., an array or map) to count entries per log level (INFO, ERROR, etc.), protecting updates with a mutex. The main thread prints the final counts.  
   * *Hints:* Shared counters (e.g., int counts\[4\]). Global mutex pthread\_mutex\_t counts\_mutex. Threads read lines, determine the level, lock the mutex, increment the corresponding counter, unlock the mutex.  
4. **Concurrent Reading and Processing:**  
   * *Problem:* One thread is a "reader" that reads lines from the log file and puts them into a shared buffer or queue. Multiple "worker" threads concurrently take lines from the shared buffer/queue and process them (e.g., parse and extract data). Use synchronization (mutexes, conditional variables) for the shared buffer/queue.  
   * *Hints:* Implement a shared buffer (e.g., a circular buffer or array) or queue. Use a mutex to protect access. Use conditional variables to signal workers when data is available and signal the reader when buffer space is available.  
5. **Using Reader-Writer Locks:**  
   * *Problem:* Implement a shared data structure (e.g., a list of error messages). Create multiple "reader" threads that concurrently read and print error messages from the list. Create one or more "writer" threads that add new error messages to the list. Use a reader-writer lock (pthread\_rwlock\_t) to allow multiple readers simultaneously but only one writer at a time.  
   * *Hints:* Initialize pthread\_rwlock\_t. Reader threads acquire a read lock (pthread\_rwlock\_rdlock), access the data, and release the lock (pthread\_rwlock\_unlock). Writer threads acquire a write lock (pthread\_rwlock\_wrlock), modify the data, and release the lock.

**Hard:**

1. **Thread Pool for Structured Parsing and Aggregation:**  
   * *Problem:* Create a thread pool. Threads process log lines, parse them into a structured format (e.g., a struct containing timestamp, level, app, user, etc.). They then update shared aggregated statistics (e.g., counts per app/level, sum of durations per app, list of unique error messages) stored in shared data structures. Use appropriate synchronization (mutexes, potentially atomic operations for simple counts) to protect the shared data structures.  
   * *Hints:* Define structs for log entries and aggregation results. Implement thread-safe data structures (e.g., a hash map for app-specific stats protected by a mutex or per-app mutexes). Workers parse lines and update shared stats, using locks.  
2. **Concurrent Analysis of Error Stack Traces:**  
   * *Problem:* Create a thread pool. Threads process log lines. When an "ERROR" entry with a stack trace is found, the thread parses the stack trace concurrently to extract information (e.g., top method, file, line number, frequency of specific error types). This extracted information is stored/aggregated in shared, thread-safe data structures.  
   * *Hints:* Workers parse error messages and stack traces (string manipulation, regex). Shared data structures (e.g., map of error types to counts, map of method+line to frequency) need robust synchronization (mutexes, potentially fine-grained locking if the data structure allows).  
3. **Implementing a Thread-Safe Queue:**  
   * *Problem:* Build a custom thread-safe queue data structure from scratch using mutexes and conditional variables. Implement enqueue and dequeue functions that are safe for multiple producer and consumer threads. Use this queue to pass log lines from a reader thread to multiple worker threads.  
   * *Hints:* Use a linked list or array for the queue data. A mutex protects the queue's internal state (head, tail, size). A conditional variable signals consumers when items are added. Another conditional variable can signal producers when space is available (if the queue has a fixed size).  
4. **Using Barriers for Synchronization Points:**  
   * *Problem:* Divide the log analysis into phases (e.g., Phase 1: Filter and parse lines, Phase 2: Aggregate parsed data, Phase 3: Generate report from aggregated data). Use a barrier (pthread\_barrier\_t) to ensure all threads complete one phase before any thread starts the next.  
   * *Hints:* Initialize a barrier with the number of threads. After each phase of processing, threads call pthread\_barrier\_wait(). This function blocks until all participating threads have called it, then releases them simultaneously to proceed to the next phase.  
5. **Concurrent Access to Complex Shared Data:**  
   * *Problem:* Design and implement a thread-safe data structure (e.g., a hash map or balanced tree) to store and update complex statistics (like error counts per application and method, average processing times per user) concurrently from multiple threads. This requires careful consideration of locking granularity (e.g., one global lock vs. per-bucket locks in a hash map) and potential deadlocks.  
   * *Hints:* Choose an appropriate data structure. Implement thread-safe insert, update, and lookup operations. Use mutexes or reader-writer locks. Explain the locking strategy and why it's thread-safe. Consider potential performance implications of locking.

### **Only C Programming with Processes and Threads**

Problems combining both process and thread concepts in C.

**Easy:**

1. **Process per Log Level, Thread per Line (within Process):**  
   * *Problem:* Create a parent process that fork()s N child processes (e.g., one for each log level). Within *each* child process, create a separate thread for *each* line of the log file. Each thread checks if its line matches the child process's assigned level and prints it. (Highly inefficient, but demonstrates combination).  
   * *Hints:* Parent fork()s. Children receive level via exec. Children open log file, read lines in a loop, and for each line, pthread\_create() a thread. Thread checks level and prints. Child waits for all its threads. Parent waits for all children.  
2. **Parent Spawns Workers, Workers Use Threads:**  
   * *Problem:* The parent process fork()s a few child processes (e.g., 2 or 3). Each child process then creates a few threads (e.g., 2 or 3 threads per child). Each thread reads and processes a *different* (pre-split) log file section.  
   * *Hints:* Parent splits the file (or determines sections). Parent fork()s children, passing section info via exec. Children receive section info, then loop, pthread\_create()ing threads. Each thread processes its assigned section. Children pthread\_join() their threads. Parent wait()s for its children.  
3. **Basic IPC (Pipe) and Thread Processing:**  
   * *Problem:* Parent process fork()s a child and creates a pipe. Parent reads log lines and sends them one by one through the pipe to the child. The child process has a dedicated thread that continuously reads from the pipe and prints the lines it receives.  
   * *Hints:* Parent pipe(), fork(). Parent closes read end, reads log, write()s lines to pipe, closes write end. Child closes write end, pthread\_create()s a reader thread. Reader thread reads from pipe's read end in a loop until EOF, prints lines. Child pthread\_join()s the reader thread.  
4. **Process for Reading, Process for Writing, Threads for Processing:**  
   * *Problem:* Create three processes: a reader, a processor, and a writer. The reader process reads the log file and sends lines to the processor via a pipe. The processor process has multiple threads that receive lines from the pipe, filter or transform them, and send results to the writer process via another pipe. The writer process receives results and writes them to an output file.  
   * *Hints:* Parent process fork()s three children. Create two pipes. Connect reader-\>processor and processor-\>writer using pipes. Processor child creates worker threads. Synchronization needed for threads accessing the pipe read/write ends in the processor.  
5. **Simple Task Distribution:**  
   * *Problem:* Parent process reads log entries. Based on the log level (e.g., send ERRORs to one worker, INFOs to another), it sends the entry to a specific child process via IPC (e.g., a dedicated pipe for each child). Each child process has a single thread that handles incoming entries from its pipe.  
   * *Hints:* Parent creates multiple pipes (one for each worker child). Parent fork()s children, passing pipe FDs. Parent reads log, determines destination child based on level, write()s line to the correct pipe. Each child has a reader thread that reads from its assigned pipe.

**Medium:**

1. **Parallel File Section Processing (Processes \+ Threads):**  
   * *Problem:* Parent process determines file sections. It fork()s child processes, assigning each a section. Each child process then uses multiple threads to read and process its assigned section concurrently (e.g., counting lines, extracting basic info). Children send their aggregated *local* results back to the parent via pipes. Parent collects and combines results.  
   * *Hints:* Parent calculates sections, creates pipes, fork()s children, passes section info and pipe write end FDs. Children receive info, pthread\_create() threads to process their section. Threads process, update *local* shared data within the child process (using thread synchronization). Child aggregates thread results and writes to its pipe. Parent reads from pipes.  
2. **Process Pool with Thread Pool Workers:**  
   * *Problem:* Implement a pool of M worker processes. Each worker process contains a pool of N worker threads. The main process reads log entries and distributes them as tasks to the worker processes (e.g., via a task queue implemented using a shared FIFO or a dedicated pipe per process). Threads within the worker processes execute the tasks concurrently.  
   * *Hints:* Parent manages the process pool. IPC (FIFO or pipes) for task distribution to processes. Each child process implements a thread pool (reader thread for IPC, worker threads for tasks). Thread synchronization within each child for its thread pool's task queue.  
3. **IPC for Shared Data Updates (with Thread Synchronization):**  
   * *Problem:* Child processes process log lines and extract data (e.g., application names and error counts). They send this data to the parent process via IPC (e.g., a pipe). The parent process has a dedicated thread that reads from the pipe and updates a shared, thread-safe data structure (e.g., a map protected by a mutex) containing global statistics.  
   * *Hints:* Parent pipe(), fork() children. Children process and write() extracted data to the pipe. Parent closes child write ends, pthread\_create()s a reader thread. Reader thread reads from the pipe's read end, locks a global mutex, updates the shared data structure, unlocks. Main parent thread waits for children and the reader thread.  
4. **Concurrent Filtering and Aggregation (Processes \+ Threads):**  
   * *Problem:* Child processes filter log entries based on criteria (e.g., level, application). They send the filtered entries to the parent via IPC (e.g., pipes). The parent process uses multiple threads to concurrently aggregate statistics from the incoming filtered entries, using thread synchronization for shared data structures.  
   * *Hints:* Children filter and write() filtered lines to pipes. Parent reads from multiple pipes (using select/poll or separate reader threads per pipe). Parent's reader thread(s) put filtered data into a shared queue. Parent's worker threads take data from the queue and update shared aggregate stats (with mutexes).  
5. **Signal Handling and Thread Cleanup:**  
   * *Problem:* Implement signal handlers (e.g., for SIGINT, SIGTERM) in both parent and child processes. When a signal is received, processes should attempt to terminate their threads gracefully (e.g., using pthread\_cancel or setting flags that threads check) before the process exits. Parent's SIGCHLD handler should clean up after terminated children.  
   * *Hints:* Use sigaction. Signal handlers set flags or call pthread\_cancel. Threads need to be designed to be cancelable (e.g., using pthread\_setcancelstate, pthread\_setcanceltype, and including cancellation points). Parent's SIGCHLD handler calls waitpid and potentially signals children to exit.

**Hard:**

1. **Distributed Log Analysis with Complex Aggregation (Processes \+ Threads \+ IPC):**  
   * *Problem:* Parent process coordinates. Child processes are responsible for processing specific applications or time ranges. Each child process uses a thread pool to concurrently parse log entries, extract properties, and perform complex local aggregations (e.g., calculating error rates per method, tracking user session duration). Child processes send their aggregated results (structured data) to the parent using robust IPC (e.g., shared memory with mutexes/condvars or a dedicated message queue system). The parent process concurrently merges these results using threads and synchronization.  
   * *Hints:* Requires implementing complex IPC (shared memory or message queues) and thread-safe data structures. Parent manages the process pool and the final aggregation thread pool. Children manage their local thread pools and send structured results. Robust error handling and synchronization are crucial.  
2. **Fault-Tolerant Processing with Process/Thread Coordination:**  
   * *Problem:* Design a system where child processes or threads within children can fail. The parent process detects child failures (SIGCHLD). If a child fails, the parent needs to restart it and potentially re-assign tasks. If a thread within a child fails (more complex to detect directly at the parent level, might require child-level monitoring), the child might need to handle it (e.g., restart the thread or log an error). Tasks are distributed via IPC.  
   * *Hints:* Parent's SIGCHLD handler monitors children. Children might have their own monitoring threads or signal handlers to detect issues within their thread pool. Task distribution IPC needs to handle tasks being returned or re-assigned if a worker fails.  
3. **Real-time Log Monitoring and Processing Pipeline:**  
   * *Problem:* Implement a system that reads new log entries as they are written to the file (e.g., using inotify or tail \-f piped). The main process reads new lines, distributes them to a pool of worker processes via IPC (e.g., a shared queue). Threads within those worker processes perform low-latency analysis and update real-time statistics displayed by the main process (requires sophisticated IPC and thread synchronization for concurrent updates and display).  
   * *Hints:* Parent uses inotify or reads from a pipe connected to tail \-f. Parent manages a pool of worker processes. IPC (pipes, shared memory queue) for distributing lines to processes. Each process has a thread pool for processing. Shared memory with synchronization for real-time stats updated by worker threads and read by a display thread in the parent. Requires select/poll for handling multiple input sources.  
4. **Concurrent Stack Trace Analysis Across Processes and Threads:**  
   * *Problem:* Child processes are assigned sections of the log file. They identify error entries with stack traces. Threads within each child process concurrently parse these stack traces. Relevant information (e.g., frequency of specific error types or methods in stack traces) is aggregated locally within the child (using thread sync). Children send their aggregated stack trace analysis results to the parent via IPC. The parent merges these results concurrently using threads.  
   * *Hints:* Children implement a thread pool for parsing. Shared data structures within children for local stack trace stats (thread-safe). IPC to send local stats to parent. Parent has a thread pool to merge incoming stats from children into global stats (thread-safe).  
5. **Implementing a Distributed Task Queue (Processes \+ Threads \+ Shared Memory/FIFOs):**  
   * *Problem:* Build a robust task queue system using shared memory and process-shared synchronization (mutexes, conditional variables) or FIFOs. The main process adds log processing tasks (e.g., log line data or offsets) to the queue. A pool of worker processes, each with multiple threads, concurrently consumes tasks from the queue. Threads within workers process the tasks.  
   * *Hints:* Design the shared task queue (in shared memory or using FIFOs). Use process-shared mutexes/semaphores for shared memory queue. Worker processes have a thread pool. Threads within workers access the shared queue (with synchronization) to get tasks.

### **C Programming with Processes, Threads, and Shell Programming (C is the main controller)**

Problems where a C program manages the overall workflow, potentially using shell commands for specific tasks, and employing processes and threads for concurrency.

**Easy:**

1. **C Spawns Shell Filter, Reads Output (Pipe):**  
   * *Problem:* Write a C program that fork()s a child and creates a pipe. The child uses exec to run a shell command like grep ERROR logfile.txt. The parent reads the output of the shell command from the pipe and prints it.  
   * *Hints:* Parent pipe(), fork(). Child closes read end, redirects stdout to the write end (dup2), closes write end, execs the shell command (/bin/sh \-c "grep ERROR logfile.txt"). Parent closes write end, reads from read end, prints.  
2. **C Spawns Shell Command, Gets Exit Status:**  
   * *Problem:* Write a C program that fork()s a child. The child uses exec to run a shell command that counts lines (e.g., wc \-l logfile.txt). The C parent waits for the child and gets its exit status. (Note: wc \-l prints to stdout, exit status is usually 0 on success. Getting the count requires capturing stdout \- see Medium).  
   * *Hints:* Parent fork(), get PID. Child execs /bin/sh \-c "wc \-l logfile.txt". Parent wait(\&status), print WEXITSTATUS(status).  
3. **C Uses Threads to Process Shell Output:**  
   * *Problem:* Write a C program. It fork()s a child that runs a shell command (e.g., cat logfile.txt or grep INFO logfile.txt) and pipes its output. The parent reads from the pipe and uses a pool of threads to process the lines received from the shell command.  
   * *Hints:* Parent pipe(), fork(). Child redirects stdout to pipe write end, execs shell command. Parent closes write end, reads from pipe read end. Parent creates a thread pool and a thread-safe queue. Reader thread reads from pipe and puts lines in queue. Worker threads take from queue and process.  
4. **C Spawns Multiple Shell Filters Concurrently:**  
   * *Problem:* Write a C program that fork()s multiple children. Each child uses exec to run a different shell command to filter the log file for a specific criterion (e.g., grep INFO, grep WARNING, grep ERROR). The parent waits for all children to complete. (Getting output back requires IPC \- see Medium).  
   * *Hints:* Parent fork()s multiple times. Each child execs a different grep command. Parent wait()s for all children.  
5. **C Uses Shell for File Listing, Processes in Threads:**  
   * *Problem:* Write a C program that uses popen() to run a shell command like ls logs/. It reads the list of log files from the shell command's standard output. It then creates threads to process each listed log file (e.g., count lines).  
   * *Hints:* Use popen("ls logs/", "r"). Read lines from the FILE\* returned by popen. For each filename, create a thread to open and process that file. Use pclose().

**Medium:**

1. **C Orchestrates Shell Pipeline, Captures Output:**  
   * *Problem:* Write a C program that fork()s a child and creates a pipe. The child uses exec to run a multi-stage shell pipeline (e.g., cat logfile.txt | grep ERROR | sort \-k1). The parent reads the final output of the pipeline from the pipe and processes it (e.g., counts lines, extracts specific data).  
   * *Hints:* Parent pipe(), fork(). Child redirects stdout to pipe write end, execs /bin/sh \-c "command1 | command2 | ...". Parent reads from pipe read end.  
2. **C Distributes Work to Shell Workers (IPC):**  
   * *Problem:* Write a C program that reads log lines. Based on the content, it sends specific lines or tasks (e.g., "process line X") to worker processes via pipes. These worker processes are C programs that use exec to run shell commands to perform the actual task on the received data (e.g., a shell script that takes a log line as input and processes it).  
   * *Hints:* Parent C program creates pipes, fork()s C worker children. Parent reads log, writes tasks to pipes. Child C workers read tasks from pipes, format them as arguments for a shell script, and exec the shell script (/bin/sh script.sh "task\_data").  
3. **C Uses Threads to Manage Shell Commands:**  
   * *Problem:* Write a C program that creates multiple threads. Each thread is responsible for running a different shell command (e.g., using system() or popen()) to perform a specific analysis task on the log file (e.g., one thread runs grep INFO | wc \-l, another runs grep ERROR | awk ...). The threads collect the results of their shell commands. The main C thread aggregates the results from all threads.  
   * *Hints:* Threads use system() or popen(). If using popen, threads read from the pipe. Need thread synchronization to collect results into a shared data structure in the C program.  
4. **C Monitors File Changes (Shell \+ Signals):**  
   * *Problem:* Write a C program that uses inotify (Linux) or periodic checks (stat) to monitor the log file. When changes are detected, the C program fork()s a child that uses exec to run a shell command (tail \-n X logfile.txt) to get the new lines. The C parent reads these new lines from the child via a pipe and processes them (potentially using threads).  
   * *Hints:* C parent sets up inotify or polling loop. On change, pipe(), fork(). Child redirects stdout, execs tail. Parent reads from pipe. Parent might have worker threads to process the new lines.  
5. **C Generates Shell Scripts Dynamically:**  
   * *Problem:* Write a C program that analyzes the log file and dynamically generates small shell scripts based on the analysis (e.g., a script to list all entries for a user with more than 100 errors). The C program then fork()s a child and execs /bin/sh to run the generated script.  
   * *Hints:* C program reads log, performs analysis, writes shell script content to a temporary file or passes it as a string to /bin/sh \-c. Parent fork(), child execs /bin/sh \-c "..." or /bin/sh temp\_script.sh.

**Hard:**

1. **C as Pipeline Orchestrator with Concurrent Stages:**  
   * *Problem:* Write a C program that acts as the central orchestrator for a complex log analysis pipeline. It fork()s child processes, each of which uses exec to run a shell command or a sequence of shell commands connected by pipes. The C program connects these child processes using pipes to form the overall pipeline. Different stages of the pipeline might be handled by different child processes running different shell commands. The C program manages the flow of data through the pipes and potentially uses threads for monitoring or additional processing.  
   * *Hints:* C parent creates multiple pipes. fork() multiple children. Each child redirects its stdin/stdout to the appropriate pipe ends using dup2 before execing its shell command or pipeline stage (/bin/sh \-c "cmd1 | cmd2"). Parent closes all unused pipe ends and manages the process lifecycle. Threads could be used in the parent to read from the final stage's output pipe concurrently.  
2. **Dynamic C Process Management via Shell Scripts:**  
   * *Problem:* Write a C program that launches and monitors worker processes. Instead of the C program directly implementing the worker logic, the workers are shell scripts. The C program fork()s children and execs shell scripts, passing configuration or tasks via command-line arguments or environment variables. The C program monitors the shell script processes using waitpid and signals, potentially restarting failed script processes.  
   * *Hints:* C parent fork()s, execs shell scripts. C parent uses waitpid with WNOHANG or a SIGCHLD handler to monitor children. C parent uses kill to signal children. Shell scripts receive arguments ($1, $2, etc.) or environment variables.  
3. **Real-time Analysis Pipeline (C \+ Shell \+ IPC \+ Threads):**  
   * *Problem:* Write a C program that reads a real-time log stream (e.g., from a file using tail \-f piped in, or a network source). The C program distributes incoming log lines to a pool of worker processes via IPC (e.g., pipes). These worker processes use exec to run small shell scripts that perform initial filtering or parsing. The output of the shell scripts is piped back to the C program. The C program uses threads to concurrently process the results from the shell scripts and update real-time statistics.  
   * *Hints:* C parent reads input stream. C parent fork()s worker children. IPC (pipes) from parent to children (sending lines) and from children back to parent (shell script output). Children exec shell scripts, redirecting stdin/stdout to pipes. C parent uses select/poll to read from multiple child output pipes. C parent uses a thread pool to process results concurrently.  
4. **Hybrid Analysis with Specialized Shell Tools:**  
   * *Problem:* Write a C program that coordinates log analysis using a combination of C code and calls to external, specialized command-line tools (simulated by shell scripts). The C program reads the log, identifies sections or types of entries requiring specialized analysis, and fork()s children that exec shell scripts. These scripts use standard tools (grep, awk) and potentially custom scripts to perform the specialized task on a subset of the data. The C program collects the results from the shell scripts via pipes and integrates them.  
   * *Hints:* C program reads log, identifies tasks. C program pipe(), fork(), child redirects stdin/stdout, execs a shell script (/bin/sh script.sh). Script receives data via stdin or arguments, uses shell tools, writes results to stdout. C parent reads from pipe.  
5. **C Program as a Service with Shell/Process Workers:**  
   * *Problem:* Design a C program that runs as a background service (daemon). It monitors the log file. When certain events occur (e.g., a burst of errors), the C service fork()s a child process that uses exec to run a shell script. This script performs an automated action (e.g., sends an alert email using mail, logs to a different system using syslog, or triggers another monitoring tool). The C service manages these action processes.  
   * *Hints:* C program detaches from terminal to become a daemon. C program monitors log (inotify or polling). On event, fork(). Child closes unnecessary FDs, redirects stdin/stdout/stderr if needed, execs the shell script. C parent logs the action and potentially waits for the child or checks its exit status.

### **C Programming with Processes, Threads, and Shell Programming (C is the main controller and communication between parent and child)**

Problems where a C program is the central controller, uses processes and threads for concurrency, integrates shell commands, and utilizes explicit IPC mechanisms for communication between C processes and potentially with shell components.

**Easy:**

1. **C Spawns Shell Filter, Reads Output to Thread (Pipe):**  
   * *Problem:* Write a C program that pipe()s and fork()s a child to run a shell command (grep WARNING logfile.txt). The parent reads the output of the shell command from the pipe using a dedicated reader thread.  
   * *Hints:* Parent pipe(), fork(). Child redirects stdout to pipe write end, execs shell command. Parent closes write end, pthread\_create()s a reader thread. Reader thread reads from pipe read end and prints. Parent pthread\_join()s the reader thread and wait()s for the child process.  
2. **C Spawns Shell Command, Child Reports Status via Pipe:**  
   * *Problem:* Write a C program that pipe()s and fork()s a child to run a shell command (e.g., a script that processes data and exits with a status). The shell script worker writes its processing result (e.g., "SUCCESS" or "FAILURE") and perhaps some simple data to its stdout. The C child process captures this stdout (using popen or pipe/dup2) and sends a structured status message (e.g., "STATUS=SUCCESS RESULT=...") back to the C parent via another pipe.  
   * *Hints:* C parent creates two pipes (parent-\>child command input if needed, child-\>parent status). Parent fork()s C child. C child uses popen or pipe/dup2/exec to run the shell script and capture its stdout. C child reads shell script output, formats a status message, and write()s it to the child-\>parent pipe. C parent reads status from pipe.  
3. **C Distributes Simple Tasks to Shell Workers via Pipe:**  
   * *Problem:* Write a C program that reads log lines. For lines matching a simple pattern (e.g., "INFO"), it sends the line number or offset to a worker process via a pipe. The worker process is a C program that reads the line number/offset from the pipe and then uses exec to run a shell command (sed \-n 'Xp' logfile.txt) to extract and print that specific line.  
   * *Hints:* C parent pipe(), fork() C child worker. Parent reads log, write()s line number to pipe. Child reads line number from pipe, formats sed command, execs /bin/sh \-c "sed \-n 'Xp' logfile.txt".

**Medium:**

1. **C Orchestrates Shell Pipeline Stages, Captures Output to Threads:**  
   * *Problem:* Write a C program that pipe()s and fork()s a child process to run a multi-stage shell pipeline (cat logfile.txt | grep ERROR | sort \-k1). The C parent reads the final output of the pipeline from the pipe and uses a pool of threads to concurrently process the received lines (e.g., extract error types, count frequencies).  
   * *Hints:* Parent pipe(), fork(). Child redirects stdout to pipe write end, execs shell pipeline. Parent closes write end, creates thread pool and thread-safe queue. Reader thread reads from pipe read end and puts lines in queue. Worker threads take from queue, parse lines, update shared stats (with thread sync).  
2. **C Distributes Complex Tasks to Shell Workers (IPC \+ Threads):**  
   * *Problem:* Write a C program that reads log entries and identifies complex tasks (e.g., "analyze stack trace for entry at offset X"). It sends these task descriptions to worker processes via pipes. The worker processes are C programs that read the task description from their pipe and then use exec to run a shell script designed to handle that specific complex task. The shell script performs the analysis and writes structured results to its stdout, which the C worker captures and sends back to the C parent via another pipe. The C parent uses threads to process the incoming results from multiple workers.  
   * *Hints:* Parent C program creates pipes for task distribution and result collection. fork()s C worker children. Parent reads log, identifies complex tasks, write()s task description to worker pipe. Child reads task, execs shell script (passing task info). Shell script does complex analysis, writes structured output. Child reads shell output, write()s structured result to parent pipe. Parent has a reader thread per worker pipe or uses select/poll to read results. Parent uses worker threads to process results.  
3. **C Manages Concurrent Shell Operations with Thread Monitoring:**  
   * *Problem:* Write a C program that creates multiple threads. Each thread is responsible for launching and monitoring a separate shell command (e.g., using system() or popen()) that performs a potentially long-running analysis task on the log file. The C threads monitor the execution of their shell commands, capture their output, and report progress or completion status back to the main C thread or a shared data structure (using thread synchronization).  
   * *Hints:* C parent pthread\_create()s worker threads. Threads use system() or popen(). If using popen, threads read from the pipe. Threads update shared status variables (e.g., "running", "completed", "failed") and potentially store results in a shared buffer/queue, protected by mutexes. Main thread monitors the shared status or collects results.  
4. **C Uses Shell for Pre-processing, Threads for Parallel Analysis:**  
   * *Problem:* Write a C program that uses popen() to run a shell command that pre-processes the log file (e.g., cat logfile.txt | awk '{print $1, $3, $4, $5, $6, $7, $8}'). The C program reads the pre-processed, simplified data from the pipe and uses a pool of threads to perform parallel analysis (e.g., counting occurrences of specific events, calculating statistics).  
   * *Hints:* C parent popen()s the shell pipeline. C parent creates a thread pool and thread-safe queue. Reader thread reads simplified lines from the pipe and puts them in the queue. Worker threads take from the queue and perform analysis on the simplified data, updating shared stats (with thread sync).  
5. **C Coordinates Distributed Shell Tasks via FIFOs:**  
   * *Problem:* Write a C program that creates FIFOs for task distribution and result collection. It writes task descriptions (e.g., "analyze user X in log file Y") to a task FIFO. Multiple worker processes (which are C programs) read tasks from the task FIFO. Each worker child reads a task, uses exec to run a shell script to perform the analysis, and writes the shell script's output to a result FIFO. The coordinating C parent reads results from the result FIFO and aggregates them.  
   * *Hints:* C parent mkfifo()s task and result FIFOs. Parent writes tasks to task FIFO. Parent fork()s C worker children. Children open task FIFO (read) and result FIFO (write). Children read task, exec shell script (redirecting I/O to pipes/FIFOs), read shell output, write to result FIFO. Parent reads from result FIFO.

**Hard:**

1. **C as Dynamic Pipeline Manager with Fault Tolerance:**  
   * *Problem:* Write a C program that dynamically constructs and manages complex analysis pipelines using shell commands executed by child processes connected with pipes. The C program monitors the health of the pipeline stages (child processes) using signals (SIGCHLD). If a stage fails, the C program should detect it, log the failure, and potentially restart the failed stage or the entire pipeline, managing data flow through pipes during restarts. Threads could be used in the C parent for concurrent monitoring or data handling.  
   * *Hints:* C parent creates dynamic pipes and fork()s/execs children for pipeline stages. Parent implements a robust SIGCHLD handler to monitor children's status. Parent needs state to track the pipeline stages and their PIDs/pipe FDs. On failure, parent cleans up resources and potentially restarts. Threads can help manage non-blocking reads from pipes or concurrent monitoring.  
2. **Real-time Event Processing with Shell Actions (C \+ IPC \+ Threads):**  
   * *Problem:* Write a C program that monitors a real-time log stream. When specific critical events are detected (e.g., "ERROR" from a critical application), the C program immediately fork()s a child process. This child process uses exec to run a shell script that performs an urgent action (e.g., send an alert via email, trigger a notification). The C program manages a pool of these action processes, ensures they don't overload the system, and logs the actions taken. Threads could be used in the C parent for concurrent monitoring and action triggering.  
   * *Hints:* C parent reads stream. On critical event, fork(). Child execs action script. Parent manages the number of active action processes. Threads in the parent could handle monitoring the stream and triggering actions concurrently. IPC might be used to pass event details to the action script process.  
3. **C-controlled Distributed Analysis with Shell Workers and Shared State:**  
   * *Problem:* Write a C program that coordinates log analysis across multiple machines (simulated or actual, assuming network IPC is available or using FIFOs on a shared filesystem). The C program distributes tasks (e.g., analyze log file X) to worker machines by launching worker processes (potentially via SSH or a custom agent, simulated by execing a script on a local path representing a remote worker). These worker processes are C programs that use threads for local concurrency and exec shell scripts to perform parts of the analysis. Results are sent back to the coordinating C program via IPC (e.g., FIFOs or network sockets). Shared state about the distributed analysis is maintained by the coordinating C program, potentially using threads for concurrent updates.  
   * *Hints:* Coordinating C program manages worker processes (local exec or simulated remote launch). IPC for task distribution and result collection. Worker C programs have thread pools, exec shell scripts for analysis tasks, and send results back. Coordinating C program uses threads to manage incoming results and update overall state.  
4. **Dynamic Workload Balancing with Shell Workers:**  
   * *Problem:* Write a C program that reads a high-volume log stream. It maintains a pool of worker processes, which are C programs that use threads and exec shell scripts for processing. The C program dynamically adjusts the number of worker processes based on the incoming log rate and the processing speed of the workers. It distributes log lines evenly among the workers using IPC (e.g., a shared queue implemented with pipes or shared memory).  
   * *Hints:* C parent reads stream, monitors rate. C parent manages worker process pool size (fork/kill). IPC queue for distributing lines to workers. Worker C programs have thread pools, read from IPC queue, exec shell scripts to process lines, potentially send back simple completion signals.  
5. **C Service with Pluggable Shell Analysis Modules:**  
   * *Problem:* Design a C program that runs as a service. It monitors the log file. It can load and unload "analysis modules" which are implemented as shell scripts. The C service, upon detecting a relevant log entry, fork()s a child process and execs the appropriate shell script module, passing the log entry data via IPC (e.g., pipe or environment variables). The shell script performs its specific analysis or action and writes results to stdout, which the C service captures via pipe. The C service manages the execution of these modules and integrates their results.  
   * *Hints:* C service monitors log. C service has a mechanism to load/configure shell script paths (e.g., reads a config file). On event, C service pipe(), fork(). Child redirects I/O, sets environment variables or writes to stdin pipe, execs the shell script. Shell script reads input, performs analysis, writes to stdout. C parent reads from stdout pipe.